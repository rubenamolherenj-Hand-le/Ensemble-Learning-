{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical Question's"
      ],
      "metadata": {
        "id": "PslA96VmRm_X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it ?\n",
        "\n",
        "Answer;\n",
        "\n",
        "Ensemble Learning is a machine learning paradigm where multiple models (often called \"weak learners\" or \"base estimators\") are combined to solve a single prediction problem.\n",
        "\n",
        "- Key Idea: The core concept is the \"wisdom of crowds.\" By aggregating the predictions of multiple diverse models, the ensemble can correct for the individual errors (bias or variance) of single models. This typically results in a model that is more accurate, robust, and stable than any of the individual constituent models."
      ],
      "metadata": {
        "id": "zFxiAydSRwlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Bagging and Boosting?\n",
        "\n",
        "Answer:\n",
        "\n",
        "- Bagging (Bootstrap Aggregating):\n",
        "\n",
        "1. Training: Builds models independently and in parallel.\n",
        "\n",
        "2. Data: Uses bootstrap sampling (random sampling with replacement) to create different training sets for each model.\n",
        "\n",
        "3. Goal: Primarily reduces variance (overfitting).\n",
        "\n",
        "4. Example: Random Forest.\n",
        "\n",
        "- Boosting:\n",
        "\n",
        "1. Training: Builds models sequentially.\n",
        "\n",
        "2. Data: Each new model focuses on the instances that previous models misclassified (by increasing their weights).\n",
        "\n",
        "3. Goal: Primarily reduces bias (underfitting) and variance.\n",
        "\n",
        "4. Example: AdaBoost, XGBoost, Gradient Boosting"
      ],
      "metadata": {
        "id": "_-EgxqYFS8Fm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is bootstrap sampling and what role does it play in Bagging methods like Random Forest?\n",
        "\n",
        "Answer:\n",
        "\n",
        "- Bootstrap Sampling: This is a statistical technique that involves randomly sampling data from the original dataset with replacement. This means some data points may appear multiple times in a single sample, while others may be left out.\n",
        "\n",
        "- Role in Random Forest: It ensures diversity among the trees. By training each decision tree on a slightly different subset of data, the trees become less correlated. When their predictions are averaged, this decorrelation significantly reduces the overall variance of the model, preventing overfitting."
      ],
      "metadata": {
        "id": "uMeeK8VHTPLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4: What are Out-of-Bag (OOB) samples and how is OOB score used to evaluate ensemble models?\n",
        "\n",
        "Answer:\n",
        "\n",
        "- OOB Samples: When bootstrap sampling is used, approximately one-third (36.8%) of the data is left out of the training set for any given tree. These \"leftover\" data points are called Out-of-Bag (OOB) samples.\n",
        "\n",
        "- OOB Score: It is an internal validation metric. The model predicts the target value for each data point using only the trees that did not see that point during training. It acts as a built-in cross-validation method, allowing you to estimate the model's performance on unseen data without needing a separate validation set."
      ],
      "metadata": {
        "id": "FJr7U0TBTkLt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: Compare feature importance analysis in a single Decision Tree vs. a Random Forest.\n",
        "\n",
        "Answer:\n",
        "\n",
        "- Single Decision Tree: Feature importance is calculated based on how much a feature decreases the impurity (Gini or Entropy) at the split points. However, a single tree is highly sensitive to small changes in data; a feature deemed \"important\" might just be useful for a specific data quirk (high variance).\n",
        "\n",
        "- Random Forest: It averages the feature importance scores across all trees in the forest. This provides a much more robust and reliable ranking. It highlights features that are consistently predictive across different subsets of data, reducing the likelihood of selecting features that simply overfit the noise."
      ],
      "metadata": {
        "id": "uBS9w-lCT8gN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical Question's"
      ],
      "metadata": {
        "id": "NKaO-ysWUEGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 6: Write a Python program to:\n",
        "● Load the Breast Cancer dataset using sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "'''\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split data (good practice, though not strictly asked, helps validate)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train a Random Forest Classifier\n",
        "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_clf.fit(X_train, y_train)\n",
        "\n",
        "# 3. Get feature importance scores\n",
        "importances = rf_clf.feature_importances_\n",
        "feature_imp_df = pd.DataFrame({'Feature': X.columns, 'Importance': importances})\n",
        "\n",
        "# 4. Print the top 5 most important features\n",
        "top_5_features = feature_imp_df.sort_values(by='Importance', ascending=False).head(5)\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_5_features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Plc72okRv5O",
        "outputId": "8351f10e-6aa9-4530-bdf8-acd485207a69"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "                 Feature  Importance\n",
            "23            worst area    0.153892\n",
            "27  worst concave points    0.144663\n",
            "7    mean concave points    0.106210\n",
            "20          worst radius    0.077987\n",
            "6         mean concavity    0.068001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 7: Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n",
        "'''\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load Data\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split Data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 2. Train Single Decision Tree\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "dt_pred = dt_clf.predict(X_test)\n",
        "dt_acc = accuracy_score(y_test, dt_pred)\n",
        "\n",
        "# 3. Train Bagging Classifier (using Decision Tree as base)\n",
        "bag_clf = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "bag_clf.fit(X_train, y_train)\n",
        "bag_pred = bag_clf.predict(X_test)\n",
        "bag_acc = accuracy_score(y_test, bag_pred)\n",
        "\n",
        "# 4. Compare Accuracy\n",
        "print(f\"Single Decision Tree Accuracy: {dt_acc:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy:   {bag_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTf7bTVCU1fE",
        "outputId": "b7fa7caf-eeba-4946-9164-30d88e567cd1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Decision Tree Accuracy: 1.0000\n",
            "Bagging Classifier Accuracy:   1.0000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 8: Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy\n",
        "'''\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split\n",
        "from sklearn.datasets import load_wine # Using Wine dataset as example since dataset wasn't specified\n",
        "\n",
        "# Load Data (Example: Wine dataset)\n",
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Initialize Random Forest\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 2. Define Hyperparameters to tune\n",
        "param_grid = {\n",
        "    'n_estimators': [10, 50, 100],\n",
        "    'max_depth': [None, 5, 10, 20]\n",
        "}\n",
        "\n",
        "# 3. Setup GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 4. Print best parameters and final accuracy\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy:\", grid_search.best_score_)\n",
        "\n",
        "# Evaluate on test set\n",
        "best_rf = grid_search.best_estimator_\n",
        "print(\"Test Set Accuracy:\", best_rf.score(X_test, y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YKuVXtbOVW-D",
        "outputId": "e605d902-2b48-4d80-8482-7868ddf76e4d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'n_estimators': 100}\n",
            "Best Cross-Validation Accuracy: 0.9785714285714286\n",
            "Test Set Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Question 9: Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)\n",
        "'''\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1. Load Dataset\n",
        "housing = fetch_california_housing()\n",
        "X, y = housing.data, housing.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 2. Train Bagging Regressor\n",
        "bag_reg = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bag_reg.fit(X_train, y_train)\n",
        "y_pred_bag = bag_reg.predict(X_test)\n",
        "\n",
        "# 3. Train Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(n_estimators=50, random_state=42)\n",
        "rf_reg.fit(X_train, y_train)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "\n",
        "# 4. Compare MSE\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "print(f\"Bagging Regressor MSE:      {mse_bag:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aOkpdMSMVnmc",
        "outputId": "10e5e7a7-40aa-483c-c32c-cb6aecbdd612"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging Regressor MSE:      0.2573\n",
            "Random Forest Regressor MSE: 0.2573\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: You are working as a data scientist at a financial institution to predict loan default. You have access to customer demographic and transaction history data ?\n",
        "\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "\n",
        "- Choose between Bagging or Boosting\n",
        "- Handle overfitting\n",
        "- Select base models\n",
        "- Evaluate performance using cross-validation\n",
        "- Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "\n",
        "Answer:\n",
        "\n",
        "Here is the step-by-step approach formatted for response:\n",
        "\n",
        "1. Choose between Bagging or Boosting:\n",
        "\n",
        "- Decision: I would start with Gradient Boosting (e.g., XGBoost or LightGBM) but also test a Random Forest.\n",
        "\n",
        "- Reasoning: Loan data is often tabular with complex non-linear relationships. Boosting generally achieves higher accuracy on such tasks by reducing both bias and variance. However, if the data is extremely noisy, Random Forest (Bagging) might be safer to avoid overfitting.\n",
        "\n",
        "2. Handle Overfitting:\n",
        "\n",
        "- Technique: Use Regularization.\n",
        "\n",
        "- Implementation: For Random Forest, I would limit the **max depth** and increase **min samples split**. For Boosting, I would use learning rate shrinkage (small learning rate with more estimators) and early stopping (stop training when validation error stops improving).\n",
        "\n",
        "3. Select Base Models:\n",
        "\n",
        "- Choice: Decision Trees.\n",
        "\n",
        "- Reasoning: Trees handle categorical data (common in demographics) and numerical data well without requiring heavy scaling. They are also non-parametric, capturing complex patterns in transaction history.\n",
        "\n",
        "4. Evaluate Performance:\n",
        "\n",
        "- Method: Stratified K-Fold Cross-Validation.\n",
        "\n",
        "- Reasoning: Loan default is usually an imbalanced class problem (fewer defaults than non-defaults). Stratified CV ensures each fold has the same proportion of defaults as the whole dataset, providing a realistic performance metric (like ROC-AUC or F1-score) rather than just accuracy.\n",
        "\n",
        "5. Justification:\n",
        "\n",
        "- Ensemble learning is crucial here because financial decisions carry high risk. A single model might be biased towards specific demographic quirks. An ensemble averages out these biases, providing a more stable, fair, and accurate risk assessment, directly impacting the institution's profitability and risk management.\n"
      ],
      "metadata": {
        "id": "_vyWAIE-W80u"
      }
    }
  ]
}